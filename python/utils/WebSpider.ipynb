{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import asyncio\n",
    "import aiofiles\n",
    "import aiohttp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_page(session, page, base_url, headers):\n",
    "    url = f\"{base_url}catalogue/page-{page}.html\"\n",
    "    try:\n",
    "        async with session.get(url, headers=headers) as response:\n",
    "            if response.status != 200:\n",
    "                print(f\"Page {page}: Status {response.status} - Stopping\")\n",
    "                return False\n",
    "            \n",
    "            print(f\"Page {page}: Scraping (Status {response.status})\")\n",
    "            content = await response.text()\n",
    "            return await parse_and_save(content, page, base_url)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Page {page}: Error - {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def parse_and_save(html, page, base_url):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        u_book_names = soup.select(\"li > article > h3 > a\")\n",
    "        u_book_prices = soup.find_all(\"p\", \"price_color\")\n",
    "        u_image_urls = soup.find_all(\"img\", \"thumbnail\")\n",
    "        \n",
    "        book_names = [name.get(\"title\") for name in u_book_names]\n",
    "        book_prices = [price.get_text() for price in u_book_prices]\n",
    "        image_urls = [f\"{base_url}{img.get('src')}\" for img in u_image_urls]\n",
    "        \n",
    "        async with aiofiles.open(f\"Page{page}.md\", 'w', encoding='utf-8') as f:\n",
    "            for i in range(len(book_names)):\n",
    "                book_info = f\"\"\"\n",
    "<font size=\"7\">{book_names[i]}</font> *<font size=\"3\">Price: {book_prices[i]}</font>*\n",
    "\n",
    "![{book_names[i]}]({image_urls[i]})\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "                await f.write(book_info)\n",
    "        \n",
    "        print(f\"Page {page} has been Saved.\")\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Page {page}: Parsing error - {str(e)}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main(base_url, headers=\"\", start_page=1, max_concurrent=10):\n",
    "    connector = aiohttp.TCPConnector(limit=max_concurrent)\n",
    "    timeout = aiohttp.ClientTimeout(total=30)\n",
    "\n",
    "    async with aiohttp.ClientSession(\n",
    "        connector=connector, \n",
    "        timeout=timeout, \n",
    "        headers=headers\n",
    "    ) as session:\n",
    "        tasks = []\n",
    "        page = start_page\n",
    "        active_tasks = 0\n",
    "        last_valid_page = start_page - 1\n",
    "        should_continue = True\n",
    "\n",
    "        while should_continue:\n",
    "            while active_tasks < max_concurrent and should_continue:\n",
    "                task = asyncio.create_task(scrape_page(session, page, base_url, headers))\n",
    "                tasks.append(task)\n",
    "                page += 1\n",
    "                active_tasks += 1\n",
    "            \n",
    "            if not tasks:\n",
    "                break\n",
    "                \n",
    "            done, pending = await asyncio.wait(\n",
    "                tasks, \n",
    "                return_when=asyncio.FIRST_COMPLETED\n",
    "            )\n",
    "\n",
    "            for task in done:\n",
    "                try:\n",
    "                    success = await task\n",
    "                    active_tasks -= 1\n",
    "                    \n",
    "                    if success:\n",
    "                        last_valid_page = max(last_valid_page, page - active_tasks - 1)\n",
    "                    else:\n",
    "                        should_continue = False\n",
    "                        print(f\"Stopping at page {page - active_tasks - 1} due to error\")\n",
    "                        break\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Task error: {str(e)}\")\n",
    "                    active_tasks -= 1\n",
    "                    should_continue = False\n",
    "                    break\n",
    "            \n",
    "            tasks = list(pending)\n",
    "            \n",
    "            if not tasks and not should_continue:\n",
    "                break\n",
    "\n",
    "        print(f\"Last valid page: {last_valid_page}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://books.toscrape.com/\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "\n",
    "print(\"Starting asynchronous scraping...\")\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main(\n",
    "    base_url=base_url,\n",
    "    headers=headers,\n",
    "    start_page=1,\n",
    "    max_concurrent=5  \n",
    "))\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f\"Scraping completed in {duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
